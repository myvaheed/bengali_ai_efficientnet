{"cells":[{"metadata":{"_uuid":"1151a4e8-1420-4de4-ad72-98ae75ac96f0","_cell_guid":"7c0c9188-0167-45dd-bc71-0e2bd836821b","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train section","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/input/ext-libs/requirements.txt', 'r') as f:\n    print(f.read())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a7762fa-b1bf-48da-8ee6-7007a039a979","_cell_guid":"880dc374-2737-4e67-a371-e4c2e6556a39","trusted":true},"cell_type":"code","source":"!pip install ../input/validname-effnet/efficientnet_pytorch-0.6.3-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df2783b0-1776-48d2-91d5-2bc95ac011e2","_cell_guid":"6065502a-83e4-4f47-b5ed-64dc9966f5a5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport sklearn\nimport csv\nfrom tqdm.notebook import tqdm\nfrom efficientnet_pytorch import EfficientNet\nfrom efficientnet_pytorch.utils import (\n    round_filters,\n    get_same_padding_conv2d\n)\nfrom torch import optim\nfrom albumentations import (\n    PadIfNeeded,\n    HorizontalFlip,\n    VerticalFlip,    \n    CenterCrop,    \n    Crop,\n    Compose,\n    Transpose,\n    RandomRotate90,\n    ElasticTransform,\n    GridDistortion, \n    OpticalDistortion,\n    RandomSizedCrop,\n    OneOf,\n    CLAHE,\n    RandomBrightnessContrast,    \n    \n    RandomGamma,\n    ShiftScaleRotate ,\n    GaussNoise,\n    Blur,\n    MotionBlur,   \n    GaussianBlur,\n)\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6ca54f8-9aff-40ee-a2cb-8eefa511d151","_cell_guid":"acae9e5d-e042-451d-baa3-fbc06fab879a","trusted":true},"cell_type":"code","source":"CHECKPOINT_FNAME = '/kaggle/input/checkpoints/checkpoint_val.chk'\nBACKBONE = 'efficientnet-b3'\n\nHEIGHT = 137\nWIDTH = 236\nSIZE = 128\n\nN_EPOCHS = 30 \nBATCH_SIZE = 64\n\nLITE = False\nTRAIN = True\n\nRESUME = False\nif os.path.isfile(CHECKPOINT_FNAME):\n    RESUME = True\n\nTRAIN = ['/kaggle/input/feathers/train_data_00_l.feather',\n    '/kaggle/input/feathers/train_data_11_l.feather',\n    '/kaggle/input/feathers/train_data_22_l.feather',\n    '/kaggle/input/feathers/train_data_33_l.feather']\n\nTRAIN_LABELS = '/kaggle/input/bengaliai-cv19/train.csv'\n\nTEST = ['/kaggle/input/bengaliai-cv19/test_image_data_0.parquet',\n         '/kaggle/input/bengaliai-cv19/test_image_data_1.parquet',\n         '/kaggle/input/bengaliai-cv19/test_image_data_2.parquet',\n        '/kaggle/input/bengaliai-cv19/test_image_data_3.parquet']\nTEST_LABELS = '/kaggle/input/bengaliai-cv19/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f47c68a-ffe6-46e2-a161-8f78983dc756","_cell_guid":"e5c86481-38dd-4e90-9600-4b781bbeb12d","trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/iafoss/image-preprocessing-128x128\n\ndef bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size=SIZE, pad=16):\n    #crop a box around pixels large than the threshold \n    #some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n    #cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    #remove lo intensity pixels as noise\n    img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    #make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n    return cv2.resize(img,(size,size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengaliDatasetTrain(Dataset):\n\n    def __init__(self, df, labels, transform=True):\n        self.grapheme_roots = labels['grapheme_root'].values.astype(np.uint8)\n        self.vowel_diacritics = labels['vowel_diacritic'].values.astype(np.uint8)\n        self.consonant_diacritics = labels['consonant_diacritic'].values.astype(np.uint8)\n        self.data = df.iloc[:, 1:].values\n        self.transform = transform\n\n        self.aug = Compose([ \n            ShiftScaleRotate(p=1,border_mode=cv2.BORDER_CONSTANT,value =1),\n            OneOf([\n                ElasticTransform(p=0.1, alpha=1, sigma=50, alpha_affine=50,border_mode=cv2.BORDER_CONSTANT,value =1),\n                GridDistortion(distort_limit =0.05 ,border_mode=cv2.BORDER_CONSTANT,value =1, p=0.1),\n                OpticalDistortion(p=0.1, distort_limit= 0.05, shift_limit=0.2,border_mode=cv2.BORDER_CONSTANT,value =1)                  \n                ], p=0.3),\n            OneOf([\n                GaussNoise(var_limit=1.0),\n                Blur(),\n                GaussianBlur(blur_limit=3)\n                ], p=0.4),    \n            RandomGamma(p=0.8)])\n\n    def __str__(self):\n        string  = ''\n        string += '\\tlen = %d\\n'%len(self)\n        string += '\\n'\n        return string\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        grapheme_root = self.grapheme_roots[index]\n        vowel_diacritic = self.vowel_diacritics[index]\n        consonant_diacritic = self.consonant_diacritics[index]\n        \n#         img0 = 255 - self.data[index, :].reshape(HEIGHT, WIDTH).astype(np.uint8)\n#         img = (img0*(255.0/img0.max())).astype(np.uint8)\n#         img = crop_resize(img)\n        img = self.data[index, :].reshape(SIZE, SIZE)\n        if self.transform:\n            img = self.aug(image=img)['image']\n\n        return img, grapheme_root, vowel_diacritic, consonant_diacritic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not LITE:\n    try:\n        del df\n        gc.collect()\n    except NameError:\n        pass\n    labels_all = pd.read_csv(TRAIN_LABELS)\n    df = pd.read_feather(TRAIN[0])\n    for i in range(1, 4):\n        read = pd.read_feather(TRAIN[i])\n        gc.collect()\n        df = pd.concat([df, read], ignore_index=True)\n        gc.collect()\n    labels = labels_all[:len(df)]\n    gc.collect()\nelse:\n    try:\n        del df\n        gc.collect()\n    except NameError:\n        pass\n\n    labels_all = pd.read_csv(TRAIN_LABELS)\n    labels = labels_all[labels_all.grapheme_root.isin([59,60,61,62,63,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95])]\n\n    df_all = pd.read_feather(TRAIN[0])\n    df_all = df_all.loc[df_all.image_id.isin(labels.image_id.values)]\n    gc.collect()\n    \n    for i in range(1, 4):\n        read = pd.read_feather(TRAIN[i])\n        read = read.loc[read.image_id.isin(labels.image_id.values)]\n        gc.collect()\n        df_all = pd.concat([df_all, read], ignore_index=True)\n        gc.collect()\n\n    df = df_all.loc[df_all.image_id.isin(labels.image_id.values)]\n    del df_all, labels_all\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"labels shape = \", labels.shape)\nprint(\"df shape = \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/hysts/pytorch_image_classification/blob/master/optim.py\nclass LARSOptimizer(torch.optim.Optimizer):\n    def __init__(self,\n                 params,\n                 lr=0.02,\n                 momentum=0,\n                 weight_decay=0,\n                 eps=1e-9,\n                 thresh=1e-2):\n\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\n                \"Invalid weight_decay value: {}\".format(weight_decay))\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            eps=eps,\n            thresh=thresh)\n        super(LARSOptimizer, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            lr = group['lr']\n            eps = group['eps']\n            thresh = group['thresh']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad.data\n\n                weight_norm = torch.norm(p.data)\n                grad_norm = torch.norm(d_p)\n                local_lr = weight_norm / (\n                    eps + grad_norm + weight_decay * weight_norm)\n                local_lr = torch.where(weight_norm < thresh,\n                                       torch.ones_like(local_lr), local_lr)\n\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if 'momentum_buffer' not in param_state:\n                        buf = param_state[\n                            'momentum_buffer'] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(lr * local_lr, d_p)\n                    else:\n                        buf = param_state['momentum_buffer']\n                        buf.mul_(momentum).add_(lr * local_lr, d_p)\n                p.data.add_(-1.0, buf)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://arxiv.org/pdf/1811.00202.pdf\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = p\n        self.eps = eps\n    \n    def forward(self, x):\n        return (torch.mean(torch.abs(x**self.p), dim=(2,3)) + self.eps)**(1.0/self.p)\n\n\nclass EffNetTuned(nn.Module):\n    def __init__(self):\n        super(EffNetTuned, self).__init__()\n        self.eff = EfficientNet.from_name(BACKBONE)\n        # 1 channel\n        Conv2d = get_same_padding_conv2d(image_size = self.eff._global_params.image_size)\n        out_channels = round_filters(32, self.eff._global_params)\n        self.eff._conv_stem = Conv2d(1, out_channels, kernel_size=3, stride=2, bias=False)\n\n        self.eff._avg_pooling = GeM()\n        self.eff._dropout = nn.Dropout(p=0.2, inplace=False)\n        self.fc = nn.Sequential(\n            nn.Linear(in_features=1000, out_features=512),\n            nn.BatchNorm1d(512)\n        ) \n\n    def forward(self, x):\n        x = self.eff(x)\n        x = self.fc(x)\n        return x\n\nclass EffMultioutputNet(nn.Module):\n    def __init__(self):\n        super(EffMultioutputNet, self).__init__()\n        self.backbone = EffNetTuned()\n        self.cosRootClassifier = nn.Linear(in_features=512, out_features=168)\n        self.cosVowelClassifier = nn.Linear(in_features=512, out_features=11)\n        self.cosConsonantClassifier = nn.Linear(in_features=512, out_features=7)\n\n    def forward(self, x):\n        root = self.backbone(x)\n        root = self.cosRootClassifier(root)\n\n        vowel = self.backbone(x)\n        vowel = self.cosVowelClassifier(vowel)\n\n        consonant = self.backbone(x)\n        consonant = self.cosConsonantClassifier(consonant)\n\n        return root, vowel, consonant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_recall_multi(pred_graphemes, true_graphemes,pred_vowels,true_vowels,pred_consonants,true_consonants, n_grapheme=20, n_vowel=11, n_consonant=4):\n    pred_label_graphemes = torch.argmax(pred_graphemes, dim=1).cpu().numpy()\n    true_label_graphemes = true_graphemes.cpu().numpy()\n    pred_label_vowels = torch.argmax(pred_vowels, dim=1).cpu().numpy()\n    true_label_vowels = true_vowels.cpu().numpy()\n    pred_label_consonants = torch.argmax(pred_consonants, dim=1).cpu().numpy()\n    true_label_consonants = true_consonants.cpu().numpy()   \n\n    recall_grapheme = sklearn.metrics.recall_score(pred_label_graphemes, true_label_graphemes, average='macro')\n    recall_vowel = sklearn.metrics.recall_score(pred_label_vowels, true_label_vowels, average='macro')\n    recall_consonant = sklearn.metrics.recall_score(pred_label_consonants, true_label_consonants, average='macro')\n    scores = [recall_grapheme, recall_vowel, recall_consonant]\n    final_score = np.average(scores, weights=[2, 1, 1])\n    return final_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_labels , valid_labels = train_test_split(labels, test_size=0.20, shuffle=False) ## Split Labels\ntrain_df, valid_df = train_test_split(df, test_size=0.20, shuffle=False) ## split data\ndel df, labels\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = BengaliDatasetTrain(train_df ,train_labels,transform = True) \nvalid_dataset = BengaliDatasetTrain(valid_df ,valid_labels,transform = False) \ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=True)\ndel train_df, valid_df, train_labels, valid_labels \ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch = 0\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EffMultioutputNet()\noptimizer = LARSOptimizer(model.parameters(), lr=0.02, weight_decay=1e-3) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load(CHECKPOINT_FNAME)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n# last_epoch = checkpoint['epoch']\nlast_epoch = -1 # from scratch\n\nmodel = model.to(device)\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, epochs=N_EPOCHS, steps_per_epoch=len(train_loader), pct_start=0.0)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch, history):\n    model.train()\n    loss = 0.0\n    acc= 0.0\n    total = 0.0\n    running_loss = 0.0\n    running_acc = 0.0\n    running_recall = 0.0\n    for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        inputs = inputs.float().to(device)\n        labels1 = labels1.long().to(device)\n        labels2 = labels2.long().to(device)\n        labels3 = labels3.long().to(device)\n        total += len(inputs)\n        optimizer.zero_grad()\n        outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1))\n        loss1 = 2*criterion(outputs1,labels1)\n        loss2 = 1* criterion(outputs2,labels2)\n        loss3 = 1*criterion(outputs3,labels3)\n        running_loss += loss1.item()+loss2.item()+loss3.item()\n        running_recall+= macro_recall_multi(outputs1,labels1,outputs2,labels2,outputs3,labels3)\n        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n        (loss1+loss2+loss3).backward()\n        optimizer.step()\n        scheduler.step()\n        \n        \n    print(' running_recall : ', running_recall)\n    print(' running_loss : ', running_loss)\n    print(' running_acc : ', running_acc)\n    loss = running_loss/len(train_loader)\n    acc = running_acc/(len(train_loader)*3)\n    print(' train epoch : {}\\tacc : {:.2f}%'.format(epoch,running_acc/(len(train_loader)*3)))\n    print('loss : {:.4f}'.format(running_loss/len(train_loader)))\n    print('recall: {:.4f}'.format(running_recall/len(train_loader)))\n    total_train_recall = running_recall/len(train_loader)\n    torch.cuda.empty_cache()\n    gc.collect()\n    history.loc[epoch, 'train_loss'] = loss\n    history.loc[epoch,'train_acc'] = acc.cpu().numpy()\n    history.loc[epoch,'train_recall'] = total_train_recall\n    return  total_train_recall\n\ndef evaluate(epoch,history):\n    model.eval()\n    loss = 0.0\n    acc = 0.0\n    total = 0.0\n    running_loss = 0.0\n    running_acc = 0.0\n    running_recall = 0.0\n    with torch.no_grad():\n        for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n            inputs = inputs.float().to(device)\n            labels1 = labels1.long().to(device)\n            labels2 = labels2.long().to(device)\n            labels3 = labels3.long().to(device)\n            total += len(inputs)\n            outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1))\n            loss1 = 2*criterion(outputs1,labels1)\n            loss2 = criterion(outputs2,labels2)\n            loss3 = criterion(outputs3,labels3)\n            running_loss += loss1.item()+loss2.item()+loss3.item()\n            running_recall+= macro_recall_multi(outputs1,labels1,outputs2,labels2,outputs3,labels3)\n            running_acc += (outputs1.argmax(1)==labels1).float().mean()\n            running_acc += (outputs2.argmax(1)==labels2).float().mean()\n            running_acc += (outputs3.argmax(1)==labels3).float().mean()\n                \n    loss = running_loss/len(valid_loader)\n    acc = running_acc/(len(valid_loader)*3)\n    total_recall = running_recall/len(valid_loader) \n    print('val epoch: {} \\tval acc : {:.2f}%'.format(epoch,running_acc/(len(valid_loader)*3)))\n    print('loss : {:.4f}'.format(running_loss/len(valid_loader)))\n    print('recall: {:.4f}'.format(running_recall/len(valid_loader)))\n    history.loc[epoch, 'valid_loss'] = loss\n    history.loc[epoch, 'valid_acc'] = acc.cpu().numpy()\n    history.loc[epoch, 'valid_recall'] = total_recall\n    return  total_recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN:\n    valid_recall = 0.0\n    best_valid_recall = 0.0\n    torch.cuda.empty_cache()\n    gc.collect()\n    for epoch in range(last_epoch+1, N_EPOCHS):\n        torch.cuda.empty_cache()\n        gc.collect()\n        train_recall = train(epoch, history)\n        valid_recall = evaluate(epoch, history)\n        if valid_recall > best_valid_recall:\n            print(f'Validation recall has increased from:  {best_valid_recall:.4f} to: {valid_recall:.4f}. Saving checkpoint')\n            torch.save({\n                    'epoch': epoch,\n                    'model' : EffMultioutputNet()\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict()\n                },  + '_' + str(epoch)) ## Saving model weights based on best validation accuracy.\n            history.to_csv(LOG_FNAME)\n            best_valid_recall = valid_recall","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dda8a61-d277-45e2-ac0c-1ee1909147e6","_cell_guid":"babe8f6b-6c6e-46f6-bc57-585dc6eb8108","trusted":true},"cell_type":"code","source":"class BengaliDatasetTest(Dataset):\n\n    def __init__(self, fname):\n        print(fname)\n        self.df = pd.read_parquet(fname)\n        self.data = 255 - self.df.iloc[:, 1:].values.reshape(-1,HEIGHT, WIDTH).astype(np.uint8)\n\n    def __str__(self):\n        string  = ''\n        string += '\\tlen = %d\\n'%len(self)\n        string += '\\n'\n        return string\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        name = self.df.iloc[idx,0]\n        #normalize each image by its max val\n        img = (self.data[idx]*(255.0/self.data[idx].max())).astype(np.uint8)\n        img = crop_resize(img)\n        img = img.astype(np.float32)/255.0\n        return img, name","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce408b66-18c5-4d87-b782-5ed937ee01e8","_cell_guid":"07176494-44c5-4848-9acd-e715cd779233","trusted":true},"cell_type":"code","source":"labels = pd.read_csv(TEST_LABELS)\nprint(\"labels shape = \", labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b295e66-d56c-43ae-9f82-4f1728c8f5d4","_cell_guid":"e6c57361-aae4-45ef-89aa-89b020b6a42d","trusted":true},"cell_type":"code","source":"row_id,target = [],[]\nfor fname in TEST:\n    test_image = BengaliDatasetTest(fname)\n    dl = torch.utils.data.DataLoader(test_image,batch_size=128,num_workers=4,shuffle=False)\n    with torch.no_grad():\n        for x,y in tqdm(dl):\n            x = x.unsqueeze(1).float().cuda()\n            p1,p2,p3 = model(x)\n            p1 = p1.argmax(-1).view(-1).cpu()\n            p2 = p2.argmax(-1).view(-1).cpu()\n            p3 = p3.argmax(-1).view(-1).cpu()\n            for idx,name in enumerate(y):\n                row_id += [f'{name}_vowel_diacritic',f'{name}_grapheme_root',\n                           f'{name}_consonant_diacritic']\n                target += [p1[idx].item(),p2[idx].item(),p3[idx].item()]\n                \nsub_df = pd.DataFrame({'row_id': row_id, 'target': target})\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27f76ed8-c8c1-46bf-8c24-eab1b0f72c8c","_cell_guid":"59434534-be5f-459b-9ed7-4955be17edc4","trusted":true},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}